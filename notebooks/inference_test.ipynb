{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# Function to load and prepare the model for inference\n",
    "def load_model(model_name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=512,\n",
    "        dtype=None,  # Auto-detect dtype\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Function to perform inference\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][len(inputs[\"input_ids\"][0]) :], skip_special_tokens=True\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sleep model\n",
    "sleep_model_name = \"thinkersloop/sleep-llm-model\"\n",
    "sleep_model, sleep_tokenizer = load_model(sleep_model_name)\n",
    "\n",
    "# Load the car model\n",
    "car_model_name = \"thinkersloop/car-llm-model\"\n",
    "car_model, car_tokenizer = load_model(car_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts for inference\n",
    "sleep_prompt = \"What are the benefits of sleep for mental health?\"\n",
    "car_prompt = \"What were the key innovations that led to the development of the first gasoline-powered automobiles?\"\n",
    "\n",
    "# Generate responses\n",
    "sleep_response = generate_response(sleep_model, sleep_tokenizer, sleep_prompt)\n",
    "car_response = generate_response(car_model, car_tokenizer, car_prompt)\n",
    "\n",
    "# Print the responses\n",
    "print(\"Sleep Model Response:\")\n",
    "print(sleep_response)\n",
    "print(\"\\nCar Model Response:\")\n",
    "print(car_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webai_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
