{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Fine Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wabai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load unsloth/mistral-7b-v0.3-bnb-4bit as a legacy tokenizer.\n",
      "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load unsloth/mistral-7b-v0.3-bnb-4bit as a legacy tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# Function to load and prepare the model for inference\n",
    "def load_model(model_name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=512,\n",
    "        dtype=None,  # Auto-detect dtype\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the sleep model\n",
    "sleep_model_name = \"thinkersloop/sleep-llm-model\"\n",
    "sleep_model, sleep_tokenizer = load_model(sleep_model_name)\n",
    "\n",
    "# Load the car model\n",
    "car_model_name = \"thinkersloop/car-llm-model\"\n",
    "car_model, car_tokenizer = load_model(car_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=64,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]) :], skip_special_tokens=True\n",
    "        ).replace(\"\\n\", \"\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep Model Response:\n",
      "Sleep is essential for overall health, including mental health. Poor sleep quality or quantity has been linked to various mental health issues, such as anxiety, depression, and cognitive impairment. On the other hand, good sleep can improve mental health by promoting emotional regulation, memory consolidation, and stress management. In this article\n",
      "--------------------------------------------------\n",
      "Car Model Response:\n",
      "The first gasoline-powered automobiles were developed through a series of key innovations, including the development of the first practical internal combustion engine by Nicolas-Joseph Cugnot in 1769, the creation of the first modern automobile by Carl Benz in \n"
     ]
    }
   ],
   "source": [
    "# Example prompts for inference\n",
    "sleep_prompt = \"What are the benefits of sleep for mental health?\"\n",
    "car_prompt = \"What were the key innovations that led to the development of the first gasoline-powered automobiles?\"\n",
    "\n",
    "# Generate responses\n",
    "sleep_response = generate_response(sleep_model, sleep_tokenizer, sleep_prompt)\n",
    "car_response = generate_response(car_model, car_tokenizer, car_prompt)\n",
    "\n",
    "# Print the responses\n",
    "print(\"Sleep Model Response:\")\n",
    "print(sleep_response)\n",
    "print(\"-\" * 50)\n",
    "print(\"Car Model Response:\")\n",
    "print(car_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load test datasets\n",
    "sleep_test_dataset = load_dataset(\"thinkersloop/sleep-dataset-llm\", split=\"test\")\n",
    "car_test_dataset = load_dataset(\"thinkersloop/car-dataset-llm\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sleep model...\n",
      "Evaluating car model...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset:\n",
    "        messages = json.loads(example[\"messages\"])\n",
    "        prompt = messages[\"messages\"][0][\"content\"]\n",
    "        reference = messages[\"messages\"][1][\"content\"]\n",
    "        response = generate_response(model, tokenizer, prompt).replace(\"\\n\", \"\")\n",
    "        predictions.append(response)\n",
    "        references.append(reference)\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "\n",
    "# Evaluate sleep model\n",
    "print(\"Evaluating sleep model...\")\n",
    "sleep_predictions, sleep_references = evaluate_model(\n",
    "    sleep_model, sleep_tokenizer, sleep_test_dataset\n",
    ")\n",
    "\n",
    "# Evaluate car model\n",
    "print(\"Evaluating car model...\")\n",
    "car_predictions, car_references = evaluate_model(\n",
    "    car_model, car_tokenizer, car_test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Insomnia , hypersomnia , and narcolepsy are all sleep disorders that can have a significant impact on an individual â€™s quality of life . However , they differ in terms of their underlying causes , symptoms , and treatment approaches .Insomnia is a sleep disorder characterized by difficulty falling',\n",
       " 'The different stages of sleep are NREM and REM sleep . NREM sleep is further divided into stages N1 , N2 , and N3 . N1 is the lightest stage of sleep , where the person is drowsy and can be easily awakened . N2 is a',\n",
       " 'Sleep spindles and slow oscillations are two important brain wave patterns that occur during non-rapid eye movement (NREM) sleep. Sleep spindles are brief bursts of high-frequency brain activity that typically last for 0.5 to 1.5 seconds and occur every']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Insomnia , hypersomnia , and narcolepsy are distinct sleep disorders with different diagnostic criteria . Insomnia is characterized by difficulty falling asleep or staying asleep , while hypersomnia involves excessive sleepiness even after adequate sleep . Narcolepsy is a neurological disorder characterized by excessive daytime sleepiness , hallucinations , and cataplexy ( sudden loss of muscle tone ) . The diagnostic criteria for these disorders do not overlap directly , but they may co - occur in some individuals . For instance , a person with insomnia may also experience excessive daytime sleepiness , which is a symptom of hypersomnia or narcolepsy . However , a proper diagnosis would require a comprehensive evaluation by a healthcare professional .',\n",
       " 'The different stages of sleep include N1 , N2 , N3 ( also referred to as N4 ) , and REM sleep . N1 is the drowsy stage where brain waves and muscle activity begin to decrease . N2 is a light sleep stage where eye movement stops , brain wave frequency and muscle tonus decrease , and heart rate and body temperature go down . N3 or N4 are deep sleep stages where it becomes difficult to awaken someone . Every part of the body is relaxed , breathing , blood pressure , and body temperature are reduced . REM sleep is a unique state where dreams usually occur , the brain is active , and the body is paralyzed . These stages contribute to the overall sleep - wake cycle by forming 90 - minute cycles of REM and NREM sleep throughout the night , starting with N1 and progressing through N2 , N3 , N2 , and finally REM sleep .',\n",
       " 'Sleep spindles and slow oscillations contribute to memory consolidation and learning during NREM sleep by facilitating long - term memory storage . After training to learn a declarative memory task , the density of human sleep spindles is significantly higher , which is associated with spontaneously occurring wave oscillations . Slow - wave sleep , characterized by these slow oscillations , is considered important for memory consolidation , with impaired memory consolidation observed in individuals with primary insomnia . Additionally , sleep spindles and slow oscillations enhance the consolidation of emotional and spatial declarative memories . Emotions with negative salience presented as a cue during slow - wave sleep show better reactivation and enhanced consolidation , while specific reactivation of the hippocampus during slow - wave sleep is detected after a spatial learning task . The amplitude of hippocampal activity during slow - wave sleep correlates with the improvement in spatial memory performance . Furthermore , odour cues given during sleep can enhance contextual cues during this stage of sleep exclusively .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inputs = [\n",
    "    \"What are the benefits of sleep for mental health?\",\n",
    "    \"What were the key innovations that led to the development of the first gasoline-powered automobiles?\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "encoded_inputs = [\n",
    "    sleep_tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    for text in sample_inputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as python_time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def measure_inference_time(model, input_ids, num_repeats=3):\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(\n",
    "        enable_timing=True\n",
    "    )\n",
    "    timings = np.zeros((num_repeats, 1))\n",
    "\n",
    "    # GPU warm-up\n",
    "    for _ in range(3):\n",
    "        _ = model.generate(input_ids, max_new_tokens=64)\n",
    "\n",
    "    # Measure performance\n",
    "    with torch.no_grad():\n",
    "        for rep in range(num_repeats):\n",
    "            torch.cuda.synchronize()\n",
    "            starter.record()\n",
    "            _ = model.generate(input_ids, max_new_tokens=64)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "\n",
    "    return timings.flatten()\n",
    "\n",
    "\n",
    "def calculate_throughput(model, input_ids, batch_size, duration=10):\n",
    "    start_time = python_time.time()\n",
    "    count = 0\n",
    "    while python_time.time() - start_time < duration:\n",
    "        _ = model.generate(input_ids.repeat(batch_size, 1), max_new_tokens=20)\n",
    "        count += batch_size\n",
    "\n",
    "    throughput = count / duration\n",
    "    return throughput\n",
    "\n",
    "\n",
    "def measure_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 2259.29 ms\n",
      "Standard Deviation of Inference Time: 10.02 ms\n",
      "95th Percentile Latency: 2274.62 ms\n",
      "Queries Per Second (QPS): 2.80\n",
      "GPU Utilization: 98.75%\n"
     ]
    }
   ],
   "source": [
    "# Measure inference time\n",
    "inference_times = []\n",
    "for input_ids in encoded_inputs:\n",
    "    times = measure_inference_time(sleep_model, input_ids)\n",
    "    inference_times.extend(times)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "percentile_95 = np.percentile(inference_times, 95)\n",
    "\n",
    "# Measure throughput (QPS)\n",
    "batch_size = 4  # Adjust based on your GPU memory\n",
    "qps = calculate_throughput(sleep_model, encoded_inputs[0], batch_size)\n",
    "\n",
    "# Measure GPU utilization\n",
    "gpu_utilization = measure_gpu_utilization()\n",
    "\n",
    "# Print results\n",
    "print(f\"Average Inference Time: {mean_time:.2f} ms\")\n",
    "print(f\"Standard Deviation of Inference Time: {std_time:.2f} ms\")\n",
    "print(f\"95th Percentile Latency: {percentile_95:.2f} ms\")\n",
    "print(f\"Queries Per Second (QPS): {qps:.2f}\")\n",
    "print(\n",
    "    f\"GPU Utilization: {gpu_utilization:.2%}\"\n",
    "    if gpu_utilization\n",
    "    else \"GPU Utilization: N/A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests Per Minute (RPM): 168.00\n"
     ]
    }
   ],
   "source": [
    "rpm = qps * 60\n",
    "print(f\"Requests Per Minute (RPM): {rpm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep Model ROUGE Scores: {'rouge1': 0.3012691012691013, 'rouge2': 0.1251693017946506, 'rougeL': 0.18873348873348875}\n",
      "Car Model ROUGE Scores: {'rouge1': 0.33112472515337554, 'rouge2': 0.10982220912707506, 'rougeL': 0.21737507194154593}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "def calculate_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)  # Note that reference comes first here\n",
    "        scores.append(score)\n",
    "\n",
    "    # Average scores\n",
    "    avg_scores = {\n",
    "        key: sum(score[key].fmeasure for score in scores) / len(scores)\n",
    "        for key in scores[0].keys()\n",
    "    }\n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "sleep_rouge_scores = calculate_rouge(sleep_predictions, sleep_references)\n",
    "print(\"Sleep Model ROUGE Scores:\", sleep_rouge_scores)\n",
    "\n",
    "\n",
    "car_rouge_scores = calculate_rouge(car_predictions, car_references)\n",
    "print(\"Car Model ROUGE Scores:\", car_rouge_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webai_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
